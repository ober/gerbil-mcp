[
  {
    "id": "test-single-case-with-trace",
    "title": "Run a single test case with expression tracing for debugging",
    "description": "When a specific test case fails, debugging requires manually adding displayln statements to the test, rebuilding, running all tests, grepping for output, then removing the debug prints. A gerbil_run_tests mode that runs a single named test case and automatically instruments intermediate expressions with tracing would dramatically speed up test debugging. Could support: (1) filter to run a single test-case by name, (2) capture and return stdout/stderr from that specific test, (3) optionally wrap check expressions with before/after value logging.",
    "impact": "medium",
    "tags": [
      "test",
      "debug",
      "trace",
      "single-test",
      "displayln",
      "diagnostic"
    ],
    "use_case": "When a single test case fails and you need to understand intermediate values (cursor positions, parsed data, function return values) to diagnose the root cause without modifying the test file.",
    "example_scenario": "Test 'org-table: TAB skips separator rows' fails with cursor on line 0 instead of 2. Need to check: what does org-table-find-bounds return? What does org-table-next-data-line return? Where is the cursor after org-table-align? Currently requires adding 8 displayln statements, rebuilding (which takes time since gerbil test recompiles), running, grepping output, then cleaning up. With this feature: gerbil_run_tests with filter='TAB skips separator' and verbose=true would capture all output from that single test case.",
    "estimated_token_reduction": "~1500 tokens per debug iteration — eliminates edit-rebuild-grep-cleanup cycle, typically 3-4 iterations per failing test = ~5000 tokens saved",
    "votes": 1
  },
  {
    "id": "pre-signature-change-impact",
    "title": "Pre-signature-change impact report: find all call sites before editing a function",
    "description": "When changing a function's parameter list (adding, removing, or reordering args), there is no single tool that shows all affected call sites across both source and test files before you make the edit. Currently requires: (1) gerbil_check_test_arity to find test call sites, (2) gerbil_find_callers to find all source call sites, (3) gerbil_check_arity after the change to verify. A single tool that accepts a function name and new arity and returns: all test files calling it with wrong arity, all source files calling it with wrong arity, and a summary of how many sites need updating — would save 2-3 round trips per signature change.",
    "impact": "medium",
    "tags": [
      "arity",
      "signature",
      "refactor",
      "callers",
      "test",
      "impact"
    ],
    "use_case": "Before changing an exported function's parameter list — e.g. adding a context argument like `def-ctx` — to find all test and source call sites that will break, before running the build.",
    "example_scenario": "Added `def-ctx` parameter to `classify-symbol-token` (making it 4-arg instead of 3-arg). Discovered the breakage only after running tests, then had to manually update 10+ test call sites. Running gerbil_check_test_arity would have caught this, but it requires knowing to call it first. A pre-signature-change tool would proactively surface all affected sites in one call.",
    "estimated_token_reduction": "~600 tokens per signature change — eliminates separate check_test_arity + find_callers calls and the test-failure round trip",
    "votes": 0
  },
  {
    "id": "test-assertion-audit",
    "title": "Audit :std/test check assertions for common mistakes",
    "description": "A tool that statically analyzes :std/test files for common assertion mistakes that silently pass: (1) `check x ? #f` which tests if x satisfies predicate #f (always fails silently, prints warning but doesn't fail suite) — should be `check x => #f`; (2) `check x ? values` which tests truthiness but doesn't verify the specific value — flag when comparing against known types (string, list); (3) `check (fn args) => expected` where fn returns a struct but expected is a string/list (type mismatch that equal? silently fails on); (4) using pcre2-match (full match) where pcre2-search was intended (subject doesn't fully match pattern). Static analysis of check forms would catch these before running tests.",
    "impact": "medium",
    "tags": [
      "test",
      "check",
      "assertion",
      "audit",
      "silent-failure",
      "std-test"
    ],
    "use_case": "When writing or reviewing test suites to catch assertions that appear to test something but silently pass due to wrong check syntax, type mismatches, or semantic errors.",
    "example_scenario": "In gerbil-pcre, 4 test bugs were found: (1) `check (pcre2-match rx \"say hello\") ? values` — pcre2-match uses ANCHORED so \"say hello\" never matches \"hel+o\", m is #f, and `(#f ? values)` silently warns but doesn't fail the suite; (2) `check all => '(\"1\" \"3\" \"5\")` where all is a list of pcre-match structs not strings — equal? returns #f, prints warning, doesn't fail; (3) `check (vector? pos) ? values` where pos is a pair not vector — returns #f, `(#f ? values)` warns silently. A static tool scanning check forms for these patterns would have caught all 4 issues immediately.",
    "estimated_token_reduction": "~800 tokens per test file audit — eliminates the manual read-trace-diagnose cycle for each silent test failure, typically 3-5 bad assertions per file",
    "votes": 0
  },
  {
    "id": "security-scan-suppress-inline",
    "title": "Support inline suppression comments for security scanner false positives",
    "description": "Allow suppressing specific security scanner findings with inline comments like `;; gerbil-security: suppress ffi-pointer-void-type-mismatch` or `;; security-ok: reason`. The scanner currently has no suppression mechanism, so confirmed false positives (like a null-guard `if (!table)` being flagged as an `#ifdef` stub, or `open-output-string` flagged as a resource leak) generate noise on every scan. The tool should: (1) recognize suppression comments on the same or preceding line, (2) skip the finding for the specific rule ID, (3) optionally report suppressed findings in a separate section for audit purposes.",
    "impact": "medium",
    "tags": [
      "security",
      "scan",
      "suppress",
      "false-positive",
      "inline-comment"
    ],
    "use_case": "When a security scan produces false positives on legitimate code patterns. Without suppression, every scan reports the same known-safe findings, wasting tokens on triage and cluttering the output.",
    "example_scenario": "In gerbil-pcre, gerbil_security_scan reported a CRITICAL 'c-declare-ifdef-null-stub' on a line containing `if (!table || index >= name_count)` — a null-pointer guard, not an #ifdef stub. Confirmed safe via gerbil_detect_ifdef_stubs (0 stubs found). But every future scan will re-flag this line, requiring manual triage each time.",
    "estimated_token_reduction": "~300 tokens per scan invocation — eliminates re-triaging known false positives across repeated scans",
    "votes": 1
  },
  {
    "id": "build-verify-detect-stale-artifacts",
    "title": "Detect stale .ssi artifacts when gerbil_verify reports \"cannot find library module\"",
    "description": "When gerbil_verify (or gerbil_compile_check) reports \"cannot find library module :package/module\", the actual cause is often a stale .ssi file in .gerbil/lib/ or ~/.gerbil/lib/ that was compiled before recent changes. The error message is misleading because the module exists and compiles fine in isolation - it's just that the importing module is using a stale precompiled interface file. A heuristic detector could: (1) parse the \"cannot find library module :foo/bar\" error, (2) check if foo/bar.ss exists in the project or dependency, (3) check if foo/bar.ssi exists and its mtime is older than foo/bar.ss source, (4) if yes, append a diagnostic hint: \"Note: found stale foo/bar.ssi (older than source). Run 'make clean' or 'rm .gerbil/lib/foo/bar.ssi' and rebuild.\"",
    "impact": "medium",
    "tags": [
      "build",
      "verify",
      "compile-check",
      "stale",
      "ssi",
      "diagnostic"
    ],
    "use_case": "When adding a new module or modifying an existing one, and gerbil_verify on an importing module fails with \"cannot find library module\" even though the module compiles fine on its own. The actual issue is stale .ssi but the error message doesn't indicate this.",
    "example_scenario": "Added face.ss to project, modified core.ss to import :gemacs/face. Running gerbil_verify on core.ss failed with \"cannot find library module :gemacs/face\" even though gerbil_verify on face.ss passed. Tried multiple times, confused because face.ss existed and compiled. Actual fix: ran 'make clean' to remove stale core.ssi. A diagnostic hint would have saved 15+ minutes of debugging.",
    "estimated_token_reduction": "~800 tokens per occurrence - eliminates multiple rounds of gerbil_verify, gerbil_compile_check, gxc -S attempts, and eventually discovering the clean solution via trial and error",
    "votes": 0
  },
  {
    "id": "build-progress-monitor",
    "title": "Monitor long-running gerbil build progress in real-time",
    "description": "A tool that monitors an ongoing gerbil build (started as a background task) and reports compilation progress in real-time. Would parse the build output stream and extract: (1) current phase (compile, link), (2) current module being compiled, (3) estimated progress (N of M modules), (4) any errors/warnings so far, (5) total elapsed time. Could be invoked periodically to check status without blocking on build completion.",
    "impact": "medium",
    "tags": [
      "build",
      "monitor",
      "progress",
      "async",
      "background",
      "gxc"
    ],
    "use_case": "When starting a long build (30+ seconds) as a background task and wanting to check progress without waiting for completion. Useful for large projects where builds take 1-3 minutes.",
    "example_scenario": "Started a clean build that deletes .gerbil directory. Build is running in background. Want to know: is it still compiling modules or is it stuck? How far along is it? Has it reached the linking phase yet? Currently must either wait for completion or manually tail the output file and parse it visually.",
    "estimated_token_reduction": "~300 tokens per build check — eliminates manual tail/grep of output files and mental parsing of build phase",
    "votes": 0
  },
  {
    "id": "detect-parallel-build-conflicts",
    "title": "Detect and prevent conflicting parallel gerbil build invocations",
    "description": "A tool that detects when multiple gerbil build processes are running on the same project directory simultaneously and warns about potential conflicts. Would: (1) check for existing gerbil build processes using ps/pgrep, (2) check for lock files in .gerbil/, (3) compare working directories of running builds, (4) warn if conflicts detected with suggestion to kill old builds first. Could also implement a lock file mechanism to prevent parallel builds.",
    "impact": "medium",
    "tags": [
      "build",
      "parallel",
      "conflict",
      "lock",
      "race",
      "gxc",
      "concurrent"
    ],
    "use_case": "When launching multiple build commands (via background tasks, make, or different terminals) that conflict with each other. Parallel builds on the same project can corrupt .gerbil/ artifacts, cause lock errors, or produce inconsistent binaries.",
    "example_scenario": "Started a build as background task. Forgot it was running and started another build. Multiple gerbil build processes competed for .gerbil/lib/ files, causing intermittent lock errors and 'Text file busy' errors during install. Had to manually kill all gerbil processes and clean build. A detector would have warned: 'Warning: gerbil build already running (PID 12345) on this directory. Kill it first with: kill 12345'",
    "estimated_token_reduction": "~500 tokens per occurrence — eliminates manual ps/grep/kill cycles and clean rebuilds due to corrupted artifacts",
    "votes": 0
  },
  {
    "id": "macro-pattern-detector-deep-analysis",
    "title": "Deeper macro pattern detection: let* destructuring, echo wrappers, toggle commands",
    "description": "The current gerbil_macro_pattern_detector only detects 3 pattern types: repeated function definitions with similar structure, repeated hash-ref accessors, and repeated method wrappers. It uses simple structural comparison of top-level definitions. In practice, the most impactful macro refactoring patterns are NOT detected: (1) Repeated let* destructuring boilerplate at the top of functions (e.g., `(let* ((ed (current-editor app)) (text (get-text ed)) (pos (get-pos ed))) ...)` appearing 80+ times), (2) Simple delegation/alias functions `(def (f app) (g app))`, (3) Boolean toggle patterns `(set! *var* (not *var*)) (message ...)`, (4) Message-only stub functions, (5) Common subexpression patterns like `(echo-message! (app-state-echo app) ...)` repeated 2000+ times. Adding detection for these would catch the patterns that actually save the most lines. The tool found 1 pattern across 10 files; manual grep-based analysis found 15 patterns worth ~4500 lines of savings.",
    "impact": "high",
    "tags": [
      "macro",
      "pattern",
      "detector",
      "refactor",
      "let-star",
      "boilerplate",
      "toggle"
    ],
    "use_case": "When analyzing a large codebase (10K+ lines) for macro refactoring opportunities. The current tool misses the highest-impact patterns, forcing manual grep-based analysis across dozens of files.",
    "example_scenario": "Analyzing 82K-line gemacs codebase: gerbil_macro_pattern_detector found 1 pattern (digit-argument wrappers, 10 occurrences, -2 lines saved). Manual analysis with 15+ grep queries found: 2341 register-command! calls (same structure), 700+ functions with identical let* state-extraction preambles, 72 toggle-mode commands, 58 alias functions, 50 stub functions — totaling ~4500 lines of macro-reducible boilerplate. The tool should detect at least the let* preamble pattern (most common) and the simple delegation pattern.",
    "estimated_token_reduction": "~3000-5000 tokens per analysis — eliminates 15+ manual grep queries, manual counting, and manual pattern classification that took 3 parallel subagents ~10 minutes each",
    "votes": 0
  }
]
